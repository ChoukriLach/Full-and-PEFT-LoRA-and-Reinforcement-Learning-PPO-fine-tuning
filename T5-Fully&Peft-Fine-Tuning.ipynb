{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ac1e54-b841-433f-82bf-05c326cca8ad",
   "metadata": {},
   "source": [
    "## Fine-Tune T5 Model for Dialogue Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b202f68-1d56-41a9-a1ff-9a7d2a8f075e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lachhab Choukr allah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, GenerationConfig, TrainingArguments , Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c9557e-0bcd-4130-8b2d-327de219fd0b",
   "metadata": {},
   "source": [
    "## 1 - Load Dataset and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09bbf24b-74a3-4e60-86f7-64b4a739bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10f41c37-f92d-4019-a37a-b43dd65ddbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6e4736-523d-4cd5-99b6-0667e02c49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b1998a-282f-4790-b59e-c714df2d635c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49104be6-1902-4817-86fc-d80df119d8b4",
   "metadata": {},
   "source": [
    "## 2 - Test the model with Zero shot inferencing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68fec1f2-40dd-4376-acfe-c0f9d2022e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT PROMPT:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
      "\n",
      "------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I need to take a dictation for you.\n"
     ]
    }
   ],
   "source": [
    "dialogue = dataset['test']['dialogue'][0]\n",
    "summary = dataset['test']['summary'][0]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "generation_config = GenerationConfig(max_new_tokens=50)\n",
    "\n",
    "response = original_model.generate(\n",
    "        inputs, \n",
    "        generation_config=generation_config\n",
    ")[0]\n",
    "\n",
    "output = tokenizer.decode(\n",
    "    response, \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "print('------------------------------------------------')\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print('------------------------------------------------')\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e914f9d-e034-4a3e-927f-9fc30c78b3fe",
   "metadata": {},
   "source": [
    "the model struggles to summarize the dialogue compared to the baseline summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77132ff-9887-4588-8323-d8cca3aaef64",
   "metadata": {},
   "source": [
    "## 3 - Perform Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b576175-d9f1-48d4-a83f-794963e8367b",
   "metadata": {},
   "source": [
    "#### 3-1 Preprocess the Dialog-Summary Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b873900b-2299-496d-9c74-6a6d7bf7327c",
   "metadata": {},
   "source": [
    "Convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b198def7-cf46-4e18-ad59-5e43f7b61f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c378350-7134-4f7a-b0a0-9cac8bfee283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faedbef9-9261-4282-aabb-9f678117caac",
   "metadata": {},
   "source": [
    "#### 3.2 - Fine-Tune the Model with the Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da54533c-880b-4e87-a186-98eaeb647d9c",
   "metadata": {},
   "source": [
    "output_dir = f'./dialogue-summary-training'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e37a02b2-7140-43ad-9ada-1acc55b44512",
   "metadata": {},
   "source": [
    "trainer.train()\n",
    "\n",
    "Training a fully fine-tuned version of the model would take a few hours on a GPU. To save time,i download a checkpoint of the fully fine-tuned model from hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04b17c41-1037-4db8-9637-560567ee6394",
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_fine_tuned_model = T5ForConditionalGeneration.from_pretrained(\"truocpham/flan-dialogue-summary-checkpoint\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a320ad0-6148-4c94-adcd-dbc4afbe5652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT PROMPT:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "------------------------------------------------\n",
      "FULLY FINE-TUNEDMODEL GENERATION:\n",
      "#Person1# suggests #Person2# upgrading #Person2#'s system, hardware, and CD-ROM drive. #Person2# thinks it's great.\n"
     ]
    }
   ],
   "source": [
    "dialogue = dataset['test']['dialogue'][200]\n",
    "summary = dataset['test']['summary'][ 200]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "generation_config = GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    "\n",
    "response = fully_fine_tuned_model.generate(\n",
    "        inputs, \n",
    "        generation_config=generation_config\n",
    ")[0]\n",
    "\n",
    "output = tokenizer.decode(\n",
    "    response, \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "print('------------------------------------------------')\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print('------------------------------------------------')\n",
    "print(f'FULLY FINE-TUNEDMODEL GENERATION:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b57008-5a3f-440c-ac4d-e7542b12beb4",
   "metadata": {},
   "source": [
    "#### 3.3 - Evaluate the Model with ROUGE Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df0eed06-81d4-463c-8f85-dc8b7397d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72a85e55-4ba3-427a-96c4-5250292bd1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = dataset['test']['dialogue'][0:10]\n",
    "human_baseline_summaries = dataset['test']['summary'][0:10]\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "generation_config = GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    "\n",
    "for dialogue in dialogues:\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Summarize the following conversation.\n",
    "        \n",
    "        {dialogue}\n",
    "        \n",
    "        Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "    original_response = original_model.generate(\n",
    "            inputs, \n",
    "            generation_config=generation_config\n",
    "    )[0]\n",
    "    \n",
    "    original_output = tokenizer.decode(\n",
    "        original_response, \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    Fine_tuned_response = fully_fine_tuned_model.generate(\n",
    "        inputs, \n",
    "        generation_config=generation_config\n",
    "    )[0]\n",
    "    \n",
    "    Fine_tuned_output = tokenizer.decode(\n",
    "        Fine_tuned_response, \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "  \n",
    "    original_model_summaries.append(original_output)\n",
    "    instruct_model_summaries.append(Fine_tuned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea3b0b64-ec6a-45a6-8585-505077e167e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.25311151811151816, 'rouge2': 0.1118160782099401, 'rougeL': 0.22820105820105818, 'rougeLsum': 0.22311151811151814}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.4021097742688077, 'rouge2': 0.17532658321476857, 'rougeL': 0.2895240360670338, 'rougeLsum': 0.28700398181601194}\n"
     ]
    }
   ],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf15201d-c06c-4680-bb8f-8021a44365b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\n",
      "rouge1: 14.90%\n",
      "rouge2: 6.35%\n",
      "rougeL: 6.13%\n",
      "rougeLsum: 6.39%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\")\n",
    "\n",
    "improvement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(instruct_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215031f6-689c-4d52-a25f-25ff0fc7e12f",
   "metadata": {},
   "source": [
    "## 4 - Perform Parameter Efficient Fine-Tuning (PEFT/LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d1adf5-cc0b-4631-8efd-dfa0382eee22",
   "metadata": {},
   "source": [
    "#### 4.1 - Setup the PEFT/LoRA model for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "742d83cd-7bef-45b7-95eb-54531a333a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType , PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84cbf2b6-e093-4bf8-a3e2-5fed19e69e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "775cb639-2913-46d0-b2b2-b37565fd7005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, \n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd1af9dd-2d35-45cd-9f84-4ade52262c28",
   "metadata": {},
   "source": [
    "output_dir = f'./peft-dialogue-summary-training'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=1,\n",
    "    max_steps=100\n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eaa592a4-92f8-4b0f-b652-d814e09106b5",
   "metadata": {},
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "Here, the training requires much less computing and memory resources (often just a single GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e42fe8e-141e-48e2-9e97-4a0e4a6ebe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model_name='intotheverse/peft-dialogue-summary-checkpoint'\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(original_model,\n",
    "                                       instruct_model_name, \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c32b6b5-e861-4d22-8d37-b2a39a650e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT PROMPT:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "------------------------------------------------\n",
      "PEFT/LoRA FINE-TUNED MODEL GENERATION:\n",
      "#Person1# recommends adding a painting program to #Person2#'s software and upgrading hardware. #Person2# also wants to upgrade the hardware because it's outdated now. #Person1# recommends adding a CD-ROM drive too.\n"
     ]
    }
   ],
   "source": [
    "dialogue = dataset['test']['dialogue'][200]\n",
    "summary = dataset['test']['summary'][ 200]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "generation_config = GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    "\n",
    "response = peft_model.generate(\n",
    "        input_ids=inputs, \n",
    "        generation_config=generation_config\n",
    ")[0]\n",
    "\n",
    "output = tokenizer.decode(\n",
    "    response, \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "print('------------------------------------------------')\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print('------------------------------------------------')\n",
    "print(f'PEFT/LoRA FINE-TUNED MODEL GENERATION:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "684ebd50-fd23-4871-9dfe-c98e4687beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = dataset['test']['dialogue'][0:10]\n",
    "human_baseline_summaries = dataset['test']['summary'][0:10]\n",
    "\n",
    "peft_instruct_model_summaries = []\n",
    "\n",
    "generation_config = GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    "\n",
    "for dialogue in dialogues:\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Summarize the following conversation.\n",
    "        \n",
    "        {dialogue}\n",
    "        \n",
    "        Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "    peft_response = peft_model.generate(\n",
    "        input_ids=inputs, \n",
    "        generation_config=generation_config\n",
    "    )[0]\n",
    "    \n",
    "    peft_output = tokenizer.decode(\n",
    "        peft_response, \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "  \n",
    "    peft_instruct_model_summaries.append(peft_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ed96e11-5d35-43d6-9e52-9b08d78c1a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.25311151811151816, 'rouge2': 0.1118160782099401, 'rougeL': 0.22820105820105818, 'rougeLsum': 0.22311151811151814}\n",
      "PEFT INSTRUCT MODEL:\n",
      "{'rouge1': 0.37185752037136655, 'rouge2': 0.12021993961310115, 'rougeL': 0.27742463494487635, 'rougeLsum': 0.2750722521667799}\n"
     ]
    }
   ],
   "source": [
    "peft_instruct_model_results = rouge.compute(\n",
    "    predictions=peft_instruct_model_summaries,\n",
    "    references=human_baseline_summaries,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('PEFT INSTRUCT MODEL:')\n",
    "print(peft_instruct_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "323d40e9-cb59-4758-a4a6-2aa54175d763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT INSTRUCT MODEL over HUMAN BASELINE\n",
      "rouge1: 11.87%\n",
      "rouge2: 0.84%\n",
      "rougeL: 4.92%\n",
      "rougeLsum: 5.20%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of PEFT INSTRUCT MODEL over HUMAN BASELINE\")\n",
    "\n",
    "improvement = (np.array(list(peft_instruct_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(peft_instruct_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
